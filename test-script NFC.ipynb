{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from regret_bound import *\n",
    "from Environment import *\n",
    "from UserCat import UserCat\n",
    "from Product import Product\n",
    "from Greedy_optimizer import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from step3_ucb1 import *\n",
    "from step4_ucb1 import *\n",
    "from step5_ucb1 import *\n",
    "from step6_sw_ucb import *\n",
    "from Step6_CD import *\n",
    "from step7_ucb1 import *\n",
    "from Step3_TS import *\n",
    "from Step4_TS import *\n",
    "from Step5_TS import *\n",
    "from Step6_TS_sw import *\n",
    "from Step7_TS import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT DEFINITION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment fixed informations and Products definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "products=[]\n",
    "\n",
    "nameofproduct= [ #name of products\n",
    "    \"Calabazas\",\n",
    "    \"Hinojo\",\n",
    "    \"Sesamo\",\n",
    "    \"Girasol\",\n",
    "    \"Amapola\"\n",
    "]\n",
    "# Dictionary fixing the secondary products linked to\n",
    "secondary_dict= {\n",
    "    \"Calabazas\": [1,2],\n",
    "    \"Hinojo\": [0,2],\n",
    "    \"Sesamo\": [3,1],\n",
    "    \"Girasol\": [2,4],\n",
    "    \"Amapola\": [3,2]\n",
    "}\n",
    "\n",
    "# Matrix n_prod*n_prices collecting the possible prices for each product. Prices are in ascending order\n",
    "prices = [[6.5, 8, 9.5, 11],\n",
    "          [11., 12, 13, 16],\n",
    "          [20., 21, 22, 25],\n",
    "          [27., 29, 31, 37],\n",
    "          [40., 41, 44, 48]]\n",
    "# Production cost of the products\n",
    "cost = [4.75, 9.75, 13.5, 15.75, 15.75]\n",
    "\n",
    "#sarebbe interessante anche prendere da file il tutto così da cambiare tutto più facilmente\n",
    "#calcolo i margini dai cost mi sembra più sensato e anche più veloce se dobbiamo cambiare continuamente\n",
    "# Computation of margins linked to each product for a particular choice of price\n",
    "cost2 = np.tile(np.array([cost]).transpose(), (1, 4))\n",
    "margins = np.array(prices)-cost2\n",
    "# Creation of the 5 objects of Product class\n",
    "for i in range (5):\n",
    "    products.append(Product(prices[i], i, nameofproduct[i],margins[i]))\n",
    "\n",
    "# Parameter for the computation of the click probability on the SECOND secondary product\n",
    "lambda_q = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 1: Young and Inexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_1 = {\n",
    "    \"mean\": [8.5, 13, 21.5, 28, 40],\n",
    "    \"std\": [1, 1.5, 2, 2.5, 3]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_1 = [[0, 0.6, 0, 0, 0],\n",
    "                   [0.4, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0.2, 0],\n",
    "                   [0, 0, 0.3, 0, 0.1],\n",
    "                   [0, 0, 0.2, 0.4, 0]]\n",
    "prob_lambda_1 = lambda_correct(np.matrix(probabilities_1), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_1 = [15, 15, 10, 5, 5]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_1 = [2, 1, .5, .2, .1]\n",
    "\n",
    "user1 = UserCat(alphas_1, res_price_params_1, poisson_lambda_1, prob_lambda_1, 'Young and Not Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 2: Old and Inexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_2 = {\n",
    "    \"mean\": [9, 14, 25, 32, 44],\n",
    "    \"std\": [3, 1.5, 2.5, 3.5, 4]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_2 = [[0, 0.4, 0, 0, 0],\n",
    "                 [0.3, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.4, 0],\n",
    "                 [0, 0, 0.4, 0, 0.2],\n",
    "                 [0, 0, 0.4, 0.2, 0]]\n",
    "prob_lambda_2 = lambda_correct(np.matrix(probabilities_2), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_2 = [7, 12, 12, 12, 7]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_2 = [0.6, 1.1, 2, 0.9, 0.5]\n",
    "\n",
    "user2 = UserCat(alphas_2, res_price_params_2, poisson_lambda_2, prob_lambda_2, 'Old and Not Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 3: Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_3 = {\n",
    "    \"mean\": [7.5, 12, 23, 37, 49],\n",
    "    \"std\": [1.5, 1.5, 2, 4, 3.5]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_3 = [[0, 0.3, 0, 0, 0],\n",
    "                 [0.3, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.5, 0],\n",
    "                 [0, 0, 0.2, 0, 0.6],\n",
    "                 [0, 0, 0.3, 0.5, 0]]\n",
    "prob_lambda_3 = lambda_correct(np.matrix(probabilities_3), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_3 = [5, 5, 10, 15, 15]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_3 = [0.1, 0.2, 0.5, 1.5, 1.2]\n",
    "\n",
    "user3 = UserCat(alphas_3, res_price_params_3, poisson_lambda_3, prob_lambda_3, 'Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 0: Aggregated demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_0 = {\n",
    "    \"mean\": [9, 13, 22, 35, 42],\n",
    "    \"std\": [3, 1.5, 2, 2.5, 2.5]\n",
    "}\n",
    "\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_0 = [[0, 0.5, 0, 0, 0],\n",
    "                 [0.4, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.4, 0],\n",
    "                 [0, 0, 0.5, 0, 0.4],\n",
    "                 [0, 0, 0.2, 0.4, 0]]\n",
    "prob_lambda_0 = lambda_correct(np.matrix(probabilities_0), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_0 = [10, 10, 10, 10, 10]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_0 = [1.5, 1, .5, .4, .2]\n",
    "\n",
    "user0 = UserCat(alphas_0, res_price_params_0, poisson_lambda_0, prob_lambda_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON\n",
    "# probability distribution of the features\n",
    "# the following list has to be interpreted in the following way:\n",
    "# values_i is the parameter of the bernoulli for feature i; in our case feature0 is 1(0) for Expert(Not Expert)\n",
    "# while feature1 is 1(0) for Old(Young)\n",
    "feature_prob = [0.3, 0.4]\n",
    "# CASE WITH 3 USERS :\n",
    "# list of users \n",
    "users3 = [user1, user2, user3]\n",
    "feature_matrix3 = np.array([[0, 1], [2, 2]]) # values represent the label of the User Category\n",
    "env3 = Environment(users3, products, secondary_dict, feature_matrix3, feature_prob)\n",
    "\n",
    "# CASE WITH AGGREGATED USER :\n",
    "users0 = [user0]\n",
    "feature_matrix0 = np.array([[0, 0], [0, 0]])\n",
    "env = Environment(users0, products, secondary_dict, feature_matrix0, feature_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upper Bound for Cumulative Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_ts = TS_regret(env, 365, 1e-3)\n",
    "ub_ucb = ucb_regret(env, 365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of Parameters chosen for the Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Conversion Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 0\n",
    "cr = np.matrix(env.theoretical_values['conversion_rates'][user])\n",
    "print('Conversion Rates for aggregated user :\\n %s' %cr)\n",
    "\n",
    "user = 0\n",
    "cr = np.matrix(env3.theoretical_values['conversion_rates'][user])\n",
    "print('\\nConversion Rates for Young user :\\n %s' %cr)\n",
    "\n",
    "user = 1\n",
    "cr = np.matrix(env3.theoretical_values['conversion_rates'][user])\n",
    "print('\\nConversion Rates for Old user :\\n %s' %cr)\n",
    "\n",
    "user = 2\n",
    "cr = np.matrix(env3.theoretical_values['conversion_rates'][user])\n",
    "print('\\nConversion Rates for Expert user :\\n %s' %cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Optimal Reward e Optimal Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_opt_reward, aggr_opt_comb = env.optimal_reward()\n",
    "print('-------------------------- Aggregated User ----------------------------')\n",
    "print( 'Aggregated Optimal Reward : %.3f - Optimal Combination : %s' %(aggr_opt_reward, aggr_opt_comb))\n",
    "print('-------------------------- 3 classes Users ----------------------------')\n",
    "opt_rewards_array, opt_comb_list = env3.optimal_reward(Disaggregated=True)\n",
    "aggr_opt_reward, aggr_opt_comb = env3.optimal_reward()\n",
    "dis_opt_reward = np.sum(np.array(env3.user_cat_prob)*opt_rewards_array)\n",
    "print('Aggregated Optimal Reward : %.3f VS Disaggregated Optimal Reward : %.3f' %(aggr_opt_reward, dis_opt_reward))\n",
    "print('Aggregated Optimal Price combination : %s' %aggr_opt_comb)\n",
    "print('Optimal price combinations with users divided by category:')\n",
    "for i, user in enumerate(env3.users):\n",
    "    print('%s - %s - %f' %(user.category, str(opt_comb_list[i]), opt_rewards_array[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment's Functions  Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_rewards_array, opt_comb_list = env3.optimal_reward(Disaggregated=True)\n",
    "aggr_opt_reward, aggr_opt_comb = env3.optimal_reward()\n",
    "dis_opt_reward = np.sum(np.array(env3.user_cat_prob)*opt_rewards_array)\n",
    "print('Aggregated Optimal Reward : %.3f VS Disaggregated Optimal Reward : %.3f' %(aggr_opt_reward, dis_opt_reward))\n",
    "print('Aggregated Optimal Price combination : %s' %aggr_opt_comb)\n",
    "print('Optimal price combinations with users divided by category:')\n",
    "for i, user in enumerate(env3.users):\n",
    "    print('%s - %s - %f' %(user.category, str(opt_comb_list[i]), opt_rewards_array[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONAMENTO EXPECTED REWARD PER STEP 7 \n",
    "unique_partition = env3.expected_reward(price_combination=[2,3,2,1,0], group_list=[[0,0],[1,1],[0,1],[1,0]])\n",
    "base_case = env3.expected_reward(price_combination=[2,3,2,1,0])\n",
    "print('Unique Partition : %.6f  is equal to base case : %.6f' %(unique_partition, base_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "d = env3.simulate_day_context(2000, [[0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0]], np.array([[0,1],[2,2]]), \n",
    "[\"conversion_rates\", \"alpha_ratios\", \"products_sold\"])\n",
    "d['01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "a = env.simulate_day(2000, [0,0,0,0,0], [\"conversion_rates\", \"alpha_ratios\", \"products_sold\", \"graph_weights\"])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reservation Price Distribution Plot for one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index = 0\n",
    "x = np.arange(0, 60 , .01)\n",
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "color_list = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "for i in range(5):\n",
    "    res_price_distr = env.users[user_index].res_price_distr[i]\n",
    "    y = res_price_distr.pdf(x)\n",
    "    plt.plot(x, y, label = 'Product %d' %i, color = color_list[i])\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(4):\n",
    "        plt.scatter(prices[i][j], 0.025, color = color_list[i])\n",
    "\n",
    "plt.title(\"Reservation Price Distributions for the user category %s\" %env.users[user_index].category)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of Prouducts Sold Distributions Plot for a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index = 0\n",
    "x = np.arange(0, 10, 1)\n",
    "plt.figure(figsize=(12,7))\n",
    "\n",
    "color_list = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "for i in range(5):\n",
    "    pois_l = env.users[user_index].poisson_lambda[i]\n",
    "    distr = scipy.stats.poisson(pois_l)\n",
    "    y = distr.pmf(x[:-1])\n",
    "    y = np.insert(y, 0, 0)\n",
    "    plt.plot(x, y, label = 'Product %d' %i, color = color_list[i])\n",
    "    #plt.scatter(x, y, color = color_list[i], label = 'Product %d' %i)\n",
    "    #plt.vlines(x, 0, y, color = color_list[i], lw=5, alpha=0.5)\n",
    "\n",
    "plt.title(\"Number of Prouducts Sold Distributions for the user category %s\" %env.users[user_index].category)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Algorithm Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_optimizer = Greedy_optimizer(env)\n",
    "greedy_optimizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.optimal_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_3 = Greedy_optimizer(env3)\n",
    "greedy_3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew0, comb0 = greedy_3.run(group_list=[[0,0]]).values()\n",
    "rew1, comb1 = greedy_3.run(group_list=[[0,1]]).values()\n",
    "rew2, comb2 = greedy_3.run(group_list=[[1,0], [1,1]]).values()\n",
    "print('Optimal Expected Rewards : [%f, %f, %f]\\nOptimale Combinations [%s, %s, %s]' %(rew0,rew1,rew2,comb0,comb1,comb2))\n",
    "print('Total Expected Reward : %f' %(np.sum(np.array((rew0,rew1,rew2))*env3.user_cat_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = [env3.theoretical_values['conversion_rates'][2]]*2\n",
    "alpha = [env3.theoretical_values['alpha_ratios'][2]]*2\n",
    "n_prod = [env3.theoretical_values['n_prod_sold'][2]]*2\n",
    "greedy_3.run(conversion_rates=CR, alphas_ratio=alpha, n_prod=n_prod, group_list=[[1,1],[1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env3.optimal_reward(Disaggregated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, cominations = env3.optimal_reward(Disaggregated=True)\n",
    "np.sum(rewards*env3.user_cat_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 : Uncertain Convertion Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "learner_TS3 = Step3_TS(env, initial_beta_CR, learning_rate = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 3\n",
    "daily_users = 200\n",
    "n_days = 365\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS3.reward_history = []\n",
    "learner_TS3.price_comb_history = []\n",
    "learner_TS3.cr_matrix_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS3.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS3 = learner_TS3.opt_reward\n",
    "collected_rewards_TS3 = learner_TS3.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_TS', 'rb') as f: \n",
    "    learner_TS3 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS3 = learner_TS3.opt_reward\n",
    "collected_rewards_TS3 = learner_TS3.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS3 - collected_rewards_TS3, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS3 - collected_rewards_TS3, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_TS3 = np.cumsum(opt_reward_TS3 - collected_rewards_TS3, axis = 1)\n",
    "mean_cum_R_TS3 = np.mean(cum_R_TS3, axis = 0)\n",
    "std_dev_TS3 = np.std(cum_R_TS3, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS3)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS3-std_dev_TS3, mean_cum_R_TS3+std_dev_TS3, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS3, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS3, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS3 = mean_cum_R_TS3/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS3[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CR_TS3 = np.mean(learner_TS3.cr_matrix_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %str(mean_CR_TS3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 3\n",
    "daily_users = 200\n",
    "n_days = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb3 = step3_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb3.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb3\", 'wb') as f1:\n",
    "    pickle.dump(ucb3, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb3\", 'rb') as f1:\n",
    "    ucb3 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_ucb1_collected_rewards = ucb3.collected_rewards\n",
    "step3_ucb1_R = ucb3.regret\n",
    "# plot of the result\n",
    "mean_step3_ucb1_R = np.mean(step3_ucb1_R, axis=0)\n",
    "std_dev_step3_ucb1 = np.std(step3_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step3_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step3_ucb1_R-std_dev_step3_ucb1, mean_step3_ucb1_R+std_dev_step3_ucb1, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step3_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb3 = mean_step3_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb3.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb3.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb3.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 : Uncertain conversion rates, alpha ratio and number of products sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))*20\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "initial_beta_alpha = np.ones((2,5))\n",
    "initial_n_prod_data = np.ones((2,5))\n",
    "learner_TS4 = Step4_TS(env3, initial_beta_CR, initial_beta_alpha, initial_n_prod_data, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 10\n",
    "daily_users = 200\n",
    "n_days = 300\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS4.reward_history = []\n",
    "learner_TS4.price_comb_history = []\n",
    "learner_TS4.cr_matrix_list = []\n",
    "learner_TS4.alpha_ratios_list = []\n",
    "learner_TS4.n_prod_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS4.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS4 = learner_TS4.opt_reward\n",
    "collected_rewards_TS4 = learner_TS4.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step4_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step4_TS', 'rb') as f: \n",
    "    learner_TS4 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS4 = learner_TS4.opt_reward\n",
    "collected_rewards_TS4 = learner_TS4.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS4 - collected_rewards_TS4, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS4 - collected_rewards_TS4, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_TS4 = np.cumsum(opt_reward_TS4 - collected_rewards_TS4, axis = 1)\n",
    "mean_cum_R_TS4 = np.mean(cum_R_TS4, axis = 0)\n",
    "std_dev_TS4 = np.std(cum_R_TS4, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS4)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS4-std_dev_TS4, mean_cum_R_TS4+std_dev_TS4, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS4, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS4, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS4 = mean_cum_R_TS4/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS4[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_alpha_TS4 = np.mean(np.array(learner_TS4.alpha_ratios_list), axis = 0)\n",
    "mean_n_prod_TS4 = np.mean(np.array(learner_TS4.n_prod_list), axis = 0)\n",
    "mean_CR_TS4 = np.mean(learner_TS4.cr_matrix_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %mean_CR_TS4)\n",
    "print('\\nAlpha Ratios : %s' % mean_alpha_TS4)\n",
    "print('\\nMean Number of product sold : %s' %mean_n_prod_TS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))\n",
    "print('\\nAlpha Ratios : %s' %env.alpha_ratios[0] )\n",
    "print('\\nMean Number of product sold : %s' %(env.users[0].poisson_lambda+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 3\n",
    "daily_users = 200\n",
    "n_days = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb4 = step4_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb4.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb4\", 'wb') as f1:\n",
    "    pickle.dump(ucb4, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb4\", 'rb') as f1:\n",
    "    ucb4 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4_ucb1_collected_rewards = ucb4.collected_rewards\n",
    "step4_ucb1_R = ucb4.regret\n",
    "# plot of the result\n",
    "mean_step4_ucb1_R = np.mean(step4_ucb1_R, axis=0)\n",
    "std_dev_step4_ucb1 = np.std(step4_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step4_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step4_ucb1_R-std_dev_step4_ucb1, mean_step4_ucb1_R+std_dev_step4_ucb1, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step4_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb4 = mean_step4_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb4[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb4.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb4.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb4.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5 : Uncertain conversion rates and graph weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a_cr = np.ones((5,4))*20\n",
    "b_cr = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a_cr, b_cr])\n",
    "a_gw = np.ones((5,2))\n",
    "b_gw = np.ones((5,2))\n",
    "initial_beta_gw = np.array([a_gw, b_gw])\n",
    "learner_TS5 = Step5_TS(env, initial_beta_CR, initial_beta_gw, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 2\n",
    "daily_users = 200\n",
    "n_days = 300\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS5.reward_history = []\n",
    "learner_TS5.price_comb_history = []\n",
    "learner_TS5.cr_matrix_list = []\n",
    "learner_TS5.graph_weights_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS5.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS5 = learner_TS5.opt_reward\n",
    "collected_rewards_TS5 = learner_TS5.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step5_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS5, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step5_TS', 'rb') as f: \n",
    "    learner_TS5 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS5 = learner_TS5.opt_reward\n",
    "collected_rewards_TS5 = learner_TS5.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS5 - collected_rewards_TS5, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS5 - collected_rewards_TS5, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_TS5 = np.cumsum(opt_reward_TS5 - collected_rewards_TS5, axis = 1)\n",
    "mean_cum_R_TS5 = np.mean(cum_R_TS5, axis = 0)\n",
    "std_dev_TS5 = np.std(cum_R_TS5, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS5)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS5-std_dev_TS5, mean_cum_R_TS5+std_dev_TS5, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS5, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS5, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS5 = mean_cum_R_TS5/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS5[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CR_TS5 = np.mean(learner_TS5.cr_matrix_list, axis = 0)\n",
    "mean_GW_TS5 = np.mean(learner_TS5.graph_weights_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %mean_CR_TS5)\n",
    "print('\\nGraph Weights : %s' %mean_GW_TS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))\n",
    "print('\\nGraph Weights :\\n%s' %env.users[0].probabilities )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 3\n",
    "daily_users = 200\n",
    "n_days = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb5 = step5_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb5.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb5\", 'wb') as f1:\n",
    "    pickle.dump(ucb5, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb5\", 'rb') as f1:\n",
    "    ucb5 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step5_ucb1_collected_rewards = ucb5.collected_rewards\n",
    "step5_ucb1_R = ucb5.regret\n",
    "# plot of the result\n",
    "mean_step5_ucb1_R = np.mean(step5_ucb1_R, axis=0)\n",
    "std_dev_step5_ucb1 = np.std(step5_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step5_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step5_ucb1_R-std_dev_step5_ucb1, mean_step5_ucb1_R+std_dev_step5_ucb1, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step5_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Ratio with respect to theoretica upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb5 = mean_step5_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb5[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb5.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb5.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb5.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6 : Abrupt Changes in Demand Curve with Uncertain Conversion Rates and Graph Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Abrupt Changes Setting Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# ABRUPT CHANGE SETTING #\n",
    "#########################\n",
    "n_days = 365\n",
    "\n",
    "# We suppose to start in September, with relative high demand. Our hypothesis is that every season we have an\n",
    "# abrupt change in demand curve\n",
    "changes_dict ={ # BASE CASE: AUTUMN\n",
    "    90 : {\"mean\": [10.1, 13.8, 23.2, 36.4, 43.7], \"std\": [3, 1.5, 2, 2.5, 2.5]}, # WINTER\n",
    "    180 : {\"mean\": [9, 13, 22, 35, 42], \"std\": [3, 1.5, 2, 2.5, 2.5]}, # SPRING\n",
    "    270 : {\"mean\": [8.2, 11.7, 20.5, 31.6, 39.1], \"std\": [3, 1.5, 2, 2.5, 2.5]} # SUMMER\n",
    "}\n",
    "opt_reward = env.optimal_reward()[0]\n",
    "opt_reward_evolution = np.zeros(n_days)\n",
    "original_res_price_param = copy.deepcopy(env.users[0].res_price_params)\n",
    "for t in range(n_days):\n",
    "    if t in changes_dict.keys(): \n",
    "        env.abrupt_change_deterministic([changes_dict[t]])\n",
    "        opt_reward = env.optimal_reward()[0]\n",
    "    opt_reward_evolution[t] = opt_reward\n",
    "\n",
    "env.abrupt_change_deterministic([original_res_price_param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - SW UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*np.sqrt(365) # value of sliding window used in the exercise lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 3\n",
    "daily_users = 100\n",
    "\n",
    "sw = int(3*np.sqrt(n_days))\n",
    "sw_ucb = step6_sw_ucb(len(prices), len(prices[0]), prices, env, changes_dict, sw)\n",
    "for _ in range(n_runs):\n",
    "    sw_ucb.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"sw_ucb\", 'wb') as f1:\n",
    "    pickle.dump(sw_ucb, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"sw_ucb\", 'rb') as f1:\n",
    "    sw_ucb = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_ucb_collected_rewards = sw_ucb.collected_rewards\n",
    "sw_ucb_R = sw_ucb.regret\n",
    "# plot of the result\n",
    "mean_sw_ucb_R = np.mean(sw_ucb_R, axis=0)\n",
    "std_sw_ucb = np.std(sw_ucb_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_sw_ucb_R)\n",
    "plt.fill_between(range(n_days), mean_sw_ucb_R-std_sw_ucb, mean_sw_ucb_R+std_sw_ucb, alpha=0.4)\n",
    "changes_time = list(changes_dict.keys())\n",
    "changes_n = len(changes_time)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_sw_ucb_R)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.plot(opt_reward_evolution, color = 'green')\n",
    "plt.plot(np.mean(sw_ucb_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Change Detection UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change detection hyper parameters\n",
    "m = 50\n",
    "eps = 0.02\n",
    "h = 0.3\n",
    "cd_ucb = Step6_CD(len(prices), len(prices[0]), prices, env, changes_dict, m, eps, h)\n",
    "# run setting (n_days fixed before)\n",
    "n_runs = 3\n",
    "daily_users = 100\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    cd_ucb.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"cd_ucb\", 'wb') as f1:\n",
    "    pickle.dump(cd_ucb, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"cd_ucb\", 'rb') as f1:\n",
    "    cd_ucb = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_ucb_collected_rewards = cd_ucb.collected_rewards\n",
    "cd_ucb_R = cd_ucb.regret\n",
    "# plot of the result\n",
    "mean_cd_ucb_R = np.mean(cd_ucb_R, axis=0)\n",
    "std_cd_ucb = np.std(cd_ucb_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(mean_cd_ucb_R)\n",
    "plt.fill_between(range(n_days), mean_cd_ucb_R-std_cd_ucb, mean_cd_ucb_R+std_cd_ucb, alpha=0.4)\n",
    "changes_time = list(changes_dict.keys())\n",
    "changes_n = len(changes_time)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_cd_ucb_R)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.plot(opt_reward_evolution, color = 'green')\n",
    "plt.plot(np.mean(cd_ucb_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Sliding Window TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 3\n",
    "daily_users = 200\n",
    "n_days = 300\n",
    "\n",
    "changes_dict ={\n",
    "    #0 : {\"mean\": [9.7, 12.7, 24.5, 31.6, 41.1],\"std\": [1.5, 2, 4, 2.5, 3]},\n",
    "    100 : {\"mean\": [9.0, 11.3, 23.2, 30.1, 39.7], \"std\": [1.3, 1.7, 3.5, 2.1, 2.7]},\n",
    "    200 : {\"mean\": [10.5, 14.0, 25.8, 32.0, 43.0], \"std\": [1.7, 2.2, 4.3, 2.9, 3.2]}\n",
    "    #225 : {\"mean\": [9.7, 12.7, 24.5, 31.6, 41.1], \"std\": [1.5, 2, 4, 2.5, 3]}\n",
    "}\n",
    "changes_time = list(changes_dict.keys())\n",
    "changes_n = len(changes_time)\n",
    "\n",
    "opt_reward = env.optimal_reward()[0]\n",
    "opt_reward_evolution = np.zeros(n_days)\n",
    "original_res_price_param = copy.deepcopy(env.users[0].res_price_params)\n",
    "for t in range(n_days):\n",
    "    if t in changes_dict.keys(): \n",
    "        env.abrupt_change_deterministic([changes_dict[t]])\n",
    "        opt_reward = env.optimal_reward()[0]\n",
    "    opt_reward_evolution[t] = opt_reward\n",
    "\n",
    "env.abrupt_change_deterministic([original_res_price_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "# CONVERSION RATES INITIAL VALUES\n",
    "a_cr = np.ones((5,4))*20\n",
    "b_cr = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a_cr, b_cr])\n",
    "# GRAPH WEIGHTS INITIAL VALUES\n",
    "a_gw = np.ones((5,2))\n",
    "b_gw = np.ones((5,2))\n",
    "initial_beta_gw = np.array([a_gw, b_gw])\n",
    "# SLIDING WINDOW WIDTH\n",
    "sw = 50\n",
    "# Learner initializer\n",
    "learner_TS6 = Step6_TS_sw(env, initial_beta_CR, initial_beta_gw, sw, changes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete possible old informations form past runs \n",
    "learner_TS6.reward_history = []\n",
    "learner_TS6.price_comb_history = []\n",
    "learner_TS6.cr_matrix_list = []\n",
    "learner_TS6.graph_weights_list = []\n",
    "learner_TS6.CR_data_history = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS6.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS6 = learner_TS6.opt_reward\n",
    "collected_rewards_TS6 = learner_TS6.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step6_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS6, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step6_TS', 'rb') as f: \n",
    "    learner_TS6 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "cum_regret = np.cumsum(np.mean(opt_reward_evolution - collected_rewards_TS6, axis=0))\n",
    "plt.plot(cum_regret, 'blue', label = 'TS')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(cum_regret)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "std_regret = np.std(opt_reward_evolution - collected_rewards_TS6, axis=0)\n",
    "plt.plot(std_regret, 'blue', label = 'TS')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(std_regret)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_TS6 = np.cumsum(opt_reward_evolution - collected_rewards_TS6, axis = 1)\n",
    "mean_cum_R_TS6 = np.mean(cum_R_TS6, axis = 0)\n",
    "std_dev_TS6 = np.std(cum_R_TS6, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS6)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS6-std_dev_TS6, mean_cum_R_TS6+std_dev_TS6, alpha=0.4)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_cum_R_TS6)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.plot(opt_reward_evolution, color = 'green', label = \"Optimal Reward\")\n",
    "plt.plot(np.mean(collected_rewards_TS6, axis=0), label = \"Mean Expected Reward\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.ylim([5, 15])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7 : Context generation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "initial_beta_alpha = np.ones((2,5))\n",
    "initial_n_prod_data = np.ones((2,5))\n",
    "cg_confidence = 0.05\n",
    "learner_TS7 = Step7_TS(env3, initial_beta_CR, initial_beta_alpha, initial_n_prod_data, cg_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 4\n",
    "daily_users = 100\n",
    "n_days = 300\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS7.reward_history = []\n",
    "learner_TS7.context_history = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS7.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS7 = learner_TS7.opt_reward\n",
    "collected_rewards_TS7 = learner_TS7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step7_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS7, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step7_TS', 'rb') as f: \n",
    "    learner_TS7 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS7 = learner_TS7.opt_reward\n",
    "collected_rewards_TS7 = learner_TS7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS7 - collected_rewards_TS7, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS7 - collected_rewards_TS7, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_TS7 = np.cumsum(opt_reward_TS7 - collected_rewards_TS7, axis = 1)\n",
    "mean_cum_R_TS7 = np.mean(cum_R_TS7, axis = 0)\n",
    "std_dev_TS7 = np.std(cum_R_TS7, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS7)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS7-std_dev_TS7, mean_cum_R_TS7+std_dev_TS7, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS7, color = 'green')\n",
    "plt.axhline(aggr_opt_reward, color = 'red')\n",
    "plt.plot(np.mean(collected_rewards_TS7, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\",\"Aggregated Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 5\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "confidence = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb7 = step7_ucb1(len(prices), len(prices[0]), prices, env3, confidence)\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    ucb7.run(n_days, daily_users)\n",
    "\n",
    "opt_reward_ucb7 = ucb7.opt_reward\n",
    "collected_rewards_ucb7 = ucb7.reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb7\", 'wb') as f1:\n",
    "    pickle.dump(ucb7, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb7\", 'rb') as f1:\n",
    "    ucb7 = pickle.load(f1)\n",
    "# collect all informations for the plot\n",
    "opt_reward_ucb7 = ucb7.opt_reward\n",
    "collected_rewards_ucb7 = ucb7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_ucb7 - collected_rewards_ucb7, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_ucb7 - collected_rewards_ucb7, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"UCB1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R_ucb7 = np.cumsum(opt_reward_ucb7 - collected_rewards_ucb7, axis = 1)\n",
    "mean_cum_R_ucb7 = np.mean(cum_R_ucb7, axis = 0)\n",
    "std_dev_ucb7 = np.std(cum_R_ucb7, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_ucb7)\n",
    "plt.fill_between(range(n_days), mean_cum_R_ucb7-std_dev_ucb7, mean_cum_R_ucb7+std_dev_ucb7, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_opt_reward, _ = env3.optimal_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_ucb7, color = 'green')\n",
    "plt.axhline(aggr_opt_reward, color = 'red')\n",
    "plt.plot(np.mean(collected_rewards_ucb7, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\",\"Aggregated Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
