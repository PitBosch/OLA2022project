{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from Environment import Environment\n",
    "from UserCat import UserCat\n",
    "from Product import Product\n",
    "from Greedy_optimizer import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucb1_brute_force import *\n",
    "from ucb1_greedy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Reward test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theoretical reward : 6.786367931533913\n",
      "path reward : 8.44432783177496\n",
      "old reward : 8.015746130857606\n"
     ]
    }
   ],
   "source": [
    "# environment\n",
    "users=[]\n",
    "products=[]\n",
    "\n",
    "#name of products\n",
    "nameofproduct= [\n",
    "    \"Calabazas\",\n",
    "    \"Hinojo\",\n",
    "    \"Sesamo\",\n",
    "    \"Girasol\"\n",
    "]\n",
    "\n",
    "prices = [[4., 6, 7, 8], \n",
    "          [9., 13, 17, 21], \n",
    "          [18., 20, 22, 24], \n",
    "          [24., 26, 28, 30]]\n",
    "#1-2 di delta, Con sovrapposizione\n",
    "\n",
    "cost=[2,4.5,9,14]\n",
    "\n",
    "#sarebbe interessante anche prendere da file il tutto così da cambiare tutto più facilmente\n",
    "#calcolo i margini dai cost mi sembra più sensato e anche più veloce se dobbiamo cambiare continuamente\n",
    "\n",
    "cost2 = np.tile(np.array([cost]).transpose(), (1, 4))\n",
    "margins = np.array(prices)-cost2\n",
    "\n",
    "Secondary_dict={\"Calabazas\": [1,2], \n",
    "                \"Hinojo\": [0,2], \n",
    "                \"Sesamo\": [1,3], \n",
    "                \"Girasol\": [1,2]}\n",
    "# media è shape*scale, la varianza è shape*scale^2\n",
    "res_price_params = {\"shape\": 150, \n",
    "                    \"scale\": 0.2, \n",
    "                    \"max\" : 60, \n",
    "                    \"min\" : 5}\n",
    "\n",
    "np.random.seed(1)\n",
    "probabilities = [[0, 0.3, 0.2, 0],\n",
    "                 [0.3, 0, 0.3, 0],\n",
    "                 [0, 0.2, 0, 0.4],\n",
    "                 [0, 0.2, 0.3, 0]]\n",
    "probabilities = np.matrix(probabilities)\n",
    "\n",
    "\n",
    "alphas=[1/5, 1/5, 1/5, 1/5, 1/5] \n",
    "# per ora li generiamo così, tutti uguali -> devo generare 3 diversi vettori alpha\n",
    "\n",
    "poisson_lambda = 0.7\n",
    "#=valore atteso del numero di prodotti acquistati (specifico per prodotto)...non dipende dal\n",
    "#prodotto oltre che dallo user che dal tipo di user che\n",
    "\n",
    "# p_users = [4/9, 3/9, 2/9] #probabilità di essere un tipo di utente-> da cambiare\n",
    "p_users = [1]\n",
    "\n",
    "lambda_q = 0.5 #just my idea of lambda\n",
    "#possiamo stimarlo con i dati passati provenienti dal sito -> vino tot è stato comprato 15 volte\n",
    "\n",
    "#proviamo a pensare, ha senso vederlo come coppia? categoria-prodotto? Avrei 3 categorie *5 prodotti-> 15 lambda diversi\n",
    "# ^^ Andre: secondo me ha senso avere 3 lambda_q diversi per categoria MA non per prodotto. La probabilità di continuare\n",
    "#           a guardare secondo me dipende dall'utente e non dal prodotto\n",
    "\n",
    "for i in range (4):\n",
    "    products.append(Product(prices[i], i, nameofproduct[i],margins[i]))\n",
    "\n",
    "# for i in range(3):\n",
    "users.append(UserCat(alphas, res_price_params, poisson_lambda, probabilities))\n",
    "\n",
    "Env_prova = Environment(users, products,  lambda_q, Secondary_dict, p_users)\n",
    "\n",
    "# CASI POSSIBILI\n",
    "# Guardare documento word su trello nella scheda TEST EXPECTED REWARD\n",
    "# 0 a): b0*(1-q01)*(1-q02)*m0 \n",
    "# 0 b) : b0*q01*(1-b1)*[(1-q02)+q02*(1-b2)]*m0\n",
    "# 0-1 a) :b0*q01*b1*q12*(1-b2)*[m0+m1]\n",
    "# 0-1 b): b0*q01*b1*(1-q12)*[(1-q02)+q02*(1-b2)]*[m0+m1]\n",
    "# 0-2 a): b0*(1-q01)*q02*b2*{(1-q21)*(1-q23)+ q21*(1-b1)*[(1-q23) + q23*(1-b3)]}*[m0+m2]\n",
    "# 0-2 b) : b0*q01*(1-b1)*q02*b2*[q23*(1-b3)+(1-q23)]* [m0+m2]\n",
    "# 0-1-2 a): b0*q01*b1*q12*b2*[q23*(1-b3)*(1-q23)] *[m0+m1+m2]\n",
    "# 0-1-2 b): b0*q01*b1*(1-q12)*(q02)*b2*[q23*(1-b3)*(1-q23)] *[m0+m1+m2]\n",
    "# 0-1-2-3 a) : b0*q01*b1*q12*b2*q23*b3*[m0+m1+m2+m3]\n",
    "# 0-1-2-3 b) : b0*q01*b1*(1-q12)*q02*b2*q23*b3*[m0+m1+m2+m3]\n",
    "# 0-2-3 a) : b0*q01*(1-b1)*q02*b2*q23*b3*[m0+m2+m3]\n",
    "# 0-2-3 b) : b0*(1-q01)*q02*b2*q21*(1-b1)*q23*b3*[m0+m2+m3]\n",
    "# 0-2-3 c) : b0*(1-q01)*q02*b2*(1-q21)*q23*b3*[q31*(1-b1)+(1-q31)] *[m0+m2+m3]\n",
    "# 0-2-3-1 : b0*(1-q01) *q02*b2*q23*b3*q31*b1[m0+m1+m2+m3]\n",
    "# 0-2-1 : b0*(1-q01)*q02*b2*q12*b1*[q23*(1-b23)+(1-q23)]*[m0+m1+m2]\n",
    "\n",
    "# definisco i valori teorici\n",
    "b0 = Env_prova.theoretical_values[\"conversion_rates\"][0][0][0]\n",
    "b1 = Env_prova.theoretical_values[\"conversion_rates\"][0][1][0]\n",
    "b2 = Env_prova.theoretical_values[\"conversion_rates\"][0][2][0]\n",
    "b3 = Env_prova.theoretical_values[\"conversion_rates\"][0][3][0]\n",
    "\n",
    "q01 = Env_prova.theoretical_values[\"graph_weights\"][0][0,1]\n",
    "q02 = lambda_q * Env_prova.theoretical_values[\"graph_weights\"][0][0,2]\n",
    "q10 = Env_prova.theoretical_values[\"graph_weights\"][0][1,0]\n",
    "q12 = lambda_q * Env_prova.theoretical_values[\"graph_weights\"][0][1,2]\n",
    "q21 = Env_prova.theoretical_values[\"graph_weights\"][0][2,1]\n",
    "q23 = lambda_q * Env_prova.theoretical_values[\"graph_weights\"][0][2,3]\n",
    "q31 = Env_prova.theoretical_values[\"graph_weights\"][0][3,1]\n",
    "q32 = lambda_q * Env_prova.theoretical_values[\"graph_weights\"][0][3,2]\n",
    "\n",
    "m0 = margins[0,0] * Env_prova.theoretical_values[\"n_prod_sold\"][0]\n",
    "m1 = margins[1,0] * Env_prova.theoretical_values[\"n_prod_sold\"][0]\n",
    "m2 = margins[2,0] * Env_prova.theoretical_values[\"n_prod_sold\"][0]\n",
    "m3 = margins[3,0] * Env_prova.theoretical_values[\"n_prod_sold\"][0]\n",
    "\n",
    "theoretical_reward = ( b0*(1-q01)*(1-q02)*m0 + b0*q01*(1-b1)*((1-q02)+q02*(1-b2))*m0 + b0*q01*b1*q12*(1-b2)*(m0+m1) +\n",
    "b0*q01*b1*(1-q12)*((1-q02)+q02*(1-b2))*(m0+m1) + b0*(1-q01)*q02*b2*((1-q21)*(1-q23)+ q21*(1-b1)*((1-q23) + q23*(1-b3)))*(m0+m2) +\n",
    "b0*q01*(1-b1)*q02*b2*(q23*(1-b3)+(1-q23))* (m0+m2) + b0*q01*b1*q12*b2*(q23*(1-b3)*(1-q23)) *(m0+m1+m2) + \n",
    "b0*q01*b1*(1-q12)*(q02)*b2*(q23*(1-b3)*(1-q23)) *(m0+m1+m2) + b0*q01*b1*q12*b2*q23*b3*(m0+m1+m2+m3) +\n",
    "b0*q01*b1*(1-q12)*q02*b2*q23*b3*(m0+m1+m2+m3) + b0*(1-q01)*q02*b2*q21*(1-b1)*q23*b3*(m0+m2+m3) +\n",
    "b0*(1-q01)*q02*b2*(1-q21)*q23*b3*(q31*(1-b1)+(1-q31))*(m0+m2+m3) + b0*(1-q01) *q02*b2*q23*b3*q31*b1*(m0+m1+m2+m3)+\n",
    "b0*(1-q01)*q02*b2*q12*b1*(q23*(1-b3)+(1-q23))*(m0+m1+m2) )\n",
    "\n",
    "product = products[0]\n",
    "price_combination = [0, 0, 0, 0]\n",
    "user_index = 0\n",
    "\n",
    "Env_prova.expected_reward(price_combination)\n",
    "\n",
    "paths_list = []\n",
    "Env_prova.explore_path(paths_list, None, 0, price_combination, 0)\n",
    "algo_reward = 0. \n",
    "for path in paths_list:\n",
    "    algo_reward += path.expected_return()\n",
    "algo_reward\n",
    "\n",
    "algo_reward2 = Env_prova.product_reward(product, [], [0], [Product([], -1, \"null\", [])], price_combination, user_index)\n",
    "print(\"theoretical reward :\", theoretical_reward)\n",
    "print(\"path reward :\", algo_reward)\n",
    "print(\"old reward :\", algo_reward2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary seen : [0]\n",
      "primary bought : []\n",
      "probability : 0.0\n",
      "collected_margin : 0\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0]\n",
      "probability : 0.0\n",
      "collected_margin : 3.4\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 2]\n",
      "probability : 0.0\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 2, 3]\n",
      "probability : 0.0\n",
      "collected_margin : 35.7\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0, 2]\n",
      "probability : 0.0\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0, 1]\n",
      "primary bought : [0]\n",
      "probability : 0.0\n",
      "collected_margin : 3.4\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0, 1]\n",
      "probability : 2.1578860698312496e-10\n",
      "collected_margin : 11.049999999999999\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 1, 2]\n",
      "probability : 4.107097154041313e-05\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 1, 2, 3]\n",
      "probability : 0.008958928985301866\n",
      "collected_margin : 43.349999999999994\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0, 1, 2]\n",
      "probability : 0.035999999827369115\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0, 1]\n",
      "probability : 1.2228021062377082e-10\n",
      "collected_margin : 11.049999999999999\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 1, 2]\n",
      "probability : 2.327355053956744e-05\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 1, 2, 3]\n",
      "primary bought : [0, 1, 2, 3]\n",
      "probability : 0.005076726425004391\n",
      "collected_margin : 43.349999999999994\n",
      "\n",
      "primary seen : [0, 1, 2]\n",
      "primary bought : [0, 1, 2]\n",
      "probability : 0.020399999902175836\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 1]\n",
      "primary bought : [0, 1]\n",
      "probability : 0.2295\n",
      "collected_margin : 11.049999999999999\n",
      "\n",
      "primary seen : [0, 2]\n",
      "primary bought : [0]\n",
      "probability : 3.3567116641819435e-10\n",
      "collected_margin : 3.4\n",
      "\n",
      "primary seen : [0, 2, 1, 3]\n",
      "primary bought : [0, 2]\n",
      "probability : 0.0\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0, 2, 1, 3]\n",
      "primary bought : [0, 2, 3]\n",
      "probability : 0.0\n",
      "collected_margin : 35.7\n",
      "\n",
      "primary seen : [0, 2, 1]\n",
      "primary bought : [0, 2]\n",
      "probability : 0.0\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0, 2, 1, 3]\n",
      "primary bought : [0, 2, 1]\n",
      "probability : 1.2777635590350748e-05\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 2, 1, 3]\n",
      "primary bought : [0, 2, 1, 3]\n",
      "probability : 0.0027872223509828025\n",
      "collected_margin : 43.349999999999994\n",
      "\n",
      "primary seen : [0, 2, 1]\n",
      "primary bought : [0, 2, 1]\n",
      "probability : 0.011199999946292612\n",
      "collected_margin : 26.349999999999998\n",
      "\n",
      "primary seen : [0, 2, 3]\n",
      "primary bought : [0, 2]\n",
      "probability : 5.1110542361403e-05\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0, 2, 3, 1]\n",
      "primary bought : [0, 2, 3]\n",
      "probability : 0.0\n",
      "collected_margin : 35.7\n",
      "\n",
      "primary seen : [0, 2, 3, 1]\n",
      "primary bought : [0, 2, 3, 1]\n",
      "probability : 0.0022297778807862426\n",
      "collected_margin : 43.35\n",
      "\n",
      "primary seen : [0, 2, 3]\n",
      "primary bought : [0, 2, 3]\n",
      "probability : 0.00891911152314497\n",
      "collected_margin : 35.7\n",
      "\n",
      "primary seen : [0, 2]\n",
      "primary bought : [0, 2]\n",
      "probability : 0.044799999785170457\n",
      "collected_margin : 18.7\n",
      "\n",
      "primary seen : [0]\n",
      "primary bought : [0]\n",
      "probability : 0.63\n",
      "collected_margin : 3.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in paths_list:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING of ENVIRONMENT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "users = []\n",
    "products=[]\n",
    "\n",
    "#name of products\n",
    "nameofproduct = [\"Calabazas\", \n",
    "                 \"Hinojo\", \n",
    "                 \"Sesamo\", \n",
    "                 \"Girasol\", \n",
    "                 \"Amapola\"]\n",
    "\n",
    "prices = [[4., 5, 6, 7], \n",
    "          [6., 7, 8, 9], \n",
    "          [9., 10, 11, 12], \n",
    "          [10., 11, 12, 13], \n",
    "          [12., 13, 14, 15]]\n",
    "#1-2 di delta, Con sovrapposizione\n",
    "\n",
    "cost = [3, 5, 7, 8 ,9]\n",
    "\n",
    "#sarebbe interessante anche prendere da file il tutto così da cambiare tutto più facilmente\n",
    "#calcolo i margini dai cost mi sembra più sensato e anche più veloce se dobbiamo cambiare continuamente\n",
    "\n",
    "cost2 = np.tile(np.array([cost]).transpose(), (1, 4))\n",
    "margins = np.array(prices)-cost2\n",
    "\n",
    "Secondary_dict={\"Calabazas\": [1,4], \n",
    "                \"Hinojo\": [2,0], \n",
    "                \"Sesamo\": [0,3], \n",
    "                \"Girasol\": [4,1], \n",
    "                \"Amapola\": [3,2]}\n",
    "# media è shape*scale, la varianza è shape*scale^2\n",
    "res_price_params = {\"shape\": 4, \n",
    "                    \"scale\": 2.5}\n",
    "\n",
    "probabilities = [[0, 1, 0, 0, 1],\n",
    "                 [1, 0, 1, 0, 0],\n",
    "                 [1, 0, 0, 1, 0],\n",
    "                 [0, 1, 0, 0, 1],\n",
    "                 [0, 0, 1, 1, 0]]\n",
    "probabilities = np.matrix(probabilities)\n",
    "\n",
    "alphas=[10, 10, 10, 10, 10] \n",
    "# per ora li generiamo così, tutti uguali -> devo generare 3 diversi vettori alpha\n",
    "\n",
    "poisson_lambda = 2\n",
    "#=valore atteso del numero di prodotti acquistati (specifico per prodotto)...non dipende dal\n",
    "#prodotto oltre che dallo user che dal tipo di user che\n",
    "\n",
    "# p_users = [4/9, 3/9, 2/9] #probabilità di essere un tipo di utente-> da cambiare\n",
    "p_users = [1]\n",
    "\n",
    "lambda_q = 1\n",
    "#possiamo stimarlo con i dati passati provenienti dal sito -> vino tot è stato comprato 15 volte\n",
    "\n",
    "#proviamo a pensare, ha senso vederlo come coppia? categoria-prodotto? Avrei 3 categorie *5 prodotti-> 15 lambda diversi\n",
    "# ^^ Andre: secondo me ha senso avere 3 lambda_q diversi per categoria MA non per prodotto. La probabilità di continuare\n",
    "#           a guardare secondo me dipende dall'utente e non dal prodotto\n",
    "\n",
    "for i in range (5):\n",
    "    products.append(Product(prices[i], i, nameofproduct[i],margins[i]))\n",
    "\n",
    "# for i in range(3):\n",
    "users.append(UserCat(alphas, res_price_params, poisson_lambda, probabilities))\n",
    "\n",
    "Env = Environment(users, products,  lambda_q, Secondary_dict, p_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.522159461957445"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Env.expected_reward(price_combination = [3, 3, 3, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.943876187281173, [3, 2, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Env.optimal_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CR_vector': array([[1220., 1134.,  933.,  856.,  644.],\n",
       "        [1256., 1271., 1284., 1048., 1648.]]),\n",
       " 'alpha_ratios': array([0.278 , 0.1365, 0.162 , 0.112 , 0.3115]),\n",
       " 'n_prod_sold': array([3.04505701, 2.98145519, 2.98389085, 2.91001273, 2.90678716]),\n",
       " 'graph_weights': array([[0.        , 0.99998703, 0.        , 0.        , 0.99998459],\n",
       "        [0.99985716, 0.        , 0.99998868, 0.        , 0.        ],\n",
       "        [0.99998413, 0.        , 0.        , 0.99997753, 0.        ],\n",
       "        [0.        , 0.99995595, 0.        , 0.        , 0.9999734 ],\n",
       "        [0.        , 0.        , 0.99987015, 0.99997362, 0.        ]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Env.simulate_day(2000, [0,0,0,0,0], [\"conversion_rates\", \"alpha_ratios\", \"products_sold\", \"graph_weights\"])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.92118651, 0.85712346, 0.77872291, 0.69193743],\n",
       "        [0.77872291, 0.69193743, 0.60251972, 0.51521611],\n",
       "        [0.51521611, 0.43347012, 0.35944777, 0.29422992],\n",
       "        [0.43347012, 0.35944777, 0.29422992, 0.2380655 ],\n",
       "        [0.29422992, 0.2380655 , 0.19062241, 0.15120388]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix(Env.theoretical_values[\"conversion_rates\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Algorithm Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combination': [3, 2, 1, 1, 0], 'expected_reward': 11.943876187281173}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_optimizer = Greedy_optimizer(Env)\n",
    "greedy_optimizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = greedy_optimizer.run()['expected_reward']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 : Uncertain Convertion Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from step3_learner import TS_learner3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta = [a, b]\n",
    "learner = TS_learner3(initial_beta, Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8244/1045899204.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_days\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaily_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_runs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\step3_learner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_round, daily_users)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# Do a single iteration of the TS, and store the price combination chosen in the iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mopt_price_com\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaily_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_price_com\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\step3_learner.py\u001b[0m in \u001b[0;36miteration\u001b[1;34m(self, daily_users)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mopt_prices_combination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGreedy_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversion_rates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msampled_CR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"combination\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# 3) Fixed the prices for the day simulate the daily user iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mestimated_CR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaily_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_prices_combination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"conversion_rates\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CR_vector'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;31m# 4) Update Beta_parameters according to the simulation done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_CR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimated_CR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_prices_combination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\Environment.py\u001b[0m in \u001b[0;36msimulate_day\u001b[1;34m(self, daily_users, price_combination, to_save)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0muser_kind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_cat_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;31m# we increment the daily profit of the website by the profit done with the simulated user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_kind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprice_combination\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_save_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_kind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[1;31m# notice that we have passed only the dictionary for the specific user category sampled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\Environment.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, user, price_combination, to_save_dict)\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_res_price\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# sample which is the first product showed to the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mpage_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;31m# if alpha ratios are uncertain count each time a product is open as first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"alpha_ratios\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_save_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\UserCat.py\u001b[0m in \u001b[0;36mstart_event\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m            to visit a competitor website. It also restores the margin, preparing it for the new coming user\"\"\"\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmargin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;31m#questo è da cambiare margin rimane all'interno di\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampled_alphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_runs = 100\n",
    "daily_users = 200\n",
    "n_days = 1000\n",
    "\n",
    "# delete possible old reward_history\n",
    "learner.reward_history = []\n",
    "\n",
    "learner.run(n_days, daily_users)\n",
    "\n",
    "for i in range(n_runs) :\n",
    "    learner.run(n_days, daily_users)\n",
    "\n",
    "opt_reward = learner.opt_reward\n",
    "collected_rewards = learner.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_rewards', 'wb') as f: \n",
    "    pickle.dump(collected_rewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_rewards', 'rb') as f: \n",
    "    collected_rewards = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward - collected_rewards, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.cumsum(np.mean(opt - gr_rewards_per_experiment, axis=0)), 'g') #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.plot(np.std(opt_reward - collected_rewards, axis=0), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "#plt.plot(np.std(opt - gr_rewards_per_experiment, axis=0), 'g')  #'g' stay for green, the color for the Greedy algorithm\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_R = np.mean(R, axis=0)\n",
    "cum_R = np.cumsum(opt_reward - collected_rewards, axis = 1)\n",
    "mean_cum_R = np.mean(cum_R, axis = 0)\n",
    "std_dev = np.std(cum_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R)\n",
    "plt.fill_between(range(n_days), mean_cum_R-std_dev, mean_cum_R+std_dev, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the istant regret in the collected runs of the step 3 learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = learner.opt_reward\n",
    "(opt_reward - collected_rewards)[2][-40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.beta_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sample_CR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix(Env.theoretical_values['conversion_rates'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### ucb1 (with greedy optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_cr [[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "arms_pulled [3, 3, 3, 3, 3]\n",
      "0 \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8244/401282109.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpulled_arms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mucb_greedy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpull_arms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mestimated_CR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate_day\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdaily_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpulled_arms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"conversion_rates\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CR_vector'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mucb_greedy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpulled_arms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimated_CR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpected_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpulled_arms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mucb1g_collected_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\ucb1_greedy.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, arms_pulled, mean_of_aggregated_conversions)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marms_pulled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_of_aggregated_conversions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# I compute the mean (per product) of the aggregated conversion realizations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marms_pulled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_of_aggregated_conversions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproduct_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_products\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marms_pulled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards_per_arm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marms_pulled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\OLA2022project\\learner2.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, arms_pulled, rewards)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mproduct_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_products\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards_per_arm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marms_pulled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproduct_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpulled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marms_pulled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "ucb1_greedy_R = []\n",
    "ucb1g_collected_rewards = []\n",
    "for _ in range(n_runs):\n",
    "    ucb_greedy = ucb1_greedy(len(prices), len(prices[0]), prices, Env)\n",
    "    instant_regret = []\n",
    "    for t in range(n_days):\n",
    "        pulled_arms = ucb_greedy.pull_arms()\n",
    "        estimated_CR = Env.simulate_day(daily_users, pulled_arms, [\"conversion_rates\"])['CR_vector']\n",
    "        ucb_greedy.update(pulled_arms, estimated_CR)\n",
    "        reward = Env.expected_reward(pulled_arms)\n",
    "        ucb1g_collected_rewards.append(reward)\n",
    "        instant_regret.append(opt_reward - reward)\n",
    "    cumulative_regret = np.cumsum(instant_regret)\n",
    "    ucb1_greedy_R.append(cumulative_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb1_greedy_R\", 'wb') as f1:\n",
    "    pickle.dump(ucb1_greedy_R, f1)\n",
    "with open(\"ucb1g_collected_rewards\", 'wb') as f1:\n",
    "    pickle.dump(ucb1g_collected_rewards, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb1_greedy_R\", 'rb') as f1:\n",
    "    ucb1_greedy_R = pickle.load(f1)\n",
    "with open(\"ucb1g_collected_rewards\", 'rb') as f1:\n",
    "    ucb1g_collected_rewards = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the result\n",
    "mean_ucbg_R = np.mean(ucb1_greedy_R, axis=0)\n",
    "std_dev_ucbg = np.std(ucb1_greedy_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_ucbg_R)\n",
    "plt.fill_between(range(n_days), mean_ucbg_R-std_dev_ucbg, mean_ucbg_R+std_dev_ucbg, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(ucb1g_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### ucb1 (brute force approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb1_bforce_R = []\n",
    "ucb1bf_collected_rewards = []\n",
    "for _ in range(n_runs):\n",
    "    ucb_brute_force = ucb1_brute_force(len(prices), len(prices[0]), prices, Env)\n",
    "    instant_regret = []\n",
    "    for t in range(n_days):\n",
    "        pulled_arms = ucb_brute_force.pull_arms()\n",
    "        estimated_CR = Env.simulate_day(daily_users, pulled_arms, [\"conversion_rates\"])['CR_vector']\n",
    "        ucb_brute_force.update(pulled_arms, estimated_CR)\n",
    "        reward = Env.expected_reward(pulled_arms)\n",
    "        ucb1bf_collected_rewards.append(reward)\n",
    "        instant_regret.append(opt_reward - reward)\n",
    "    cumulative_regret = np.cumsum(instant_regret)\n",
    "    ucb1_bforce_R.append(cumulative_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb1_bforce_R\", 'wb') as f1:\n",
    "    pickle.dump(ucb1_bforce_R, f1)\n",
    "with open(\"ucb1bf_collected_rewards\", 'wb') as f1:\n",
    "    pickle.dump(ucb1bf_collected_rewards, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb1_bforce_R\", 'rb') as f1:\n",
    "    ucb1_bforce_R = pickle.load(f1)\n",
    "with open(\"ucb1bf_collected_rewards\", 'rb') as f1:\n",
    "    ucb1bf_collected_rewards = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the result\n",
    "mean_ucbbf_R = np.mean(ucb1_bforce_R, axis=0)\n",
    "std_dev_ucbbf = np.std(ucb1_bforce_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_ucbbf_R)\n",
    "plt.fill_between(range(n_days), mean_ucbbf_R-std_dev_ucbbf, mean_ucbbf_R+std_dev_ucbbf, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(ucb1bf_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
