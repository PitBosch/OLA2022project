{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from regret_bound import *\n",
    "from Environment import *\n",
    "from UserCat import UserCat\n",
    "from Product import Product\n",
    "from Greedy_optimizer import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from step3_ucb1 import *\n",
    "from step4_ucb1 import *\n",
    "from step5_ucb1 import *\n",
    "from step6_sw_ucb import *\n",
    "from Step6_CD import *\n",
    "from step7_ucb1 import *\n",
    "from Step3_TS import *\n",
    "from Step4_TS import *\n",
    "from Step5_TS import *\n",
    "from Step6_TS_sw import *\n",
    "from Step7_TS import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT DEFINITION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment fixed informations and Products definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "products=[]\n",
    "\n",
    "nameofproduct= [ #name of products\n",
    "    \"Calabazas\",\n",
    "    \"Hinojo\",\n",
    "    \"Sesamo\",\n",
    "    \"Girasol\",\n",
    "    \"Amapola\"\n",
    "]\n",
    "# Dictionary fixing the secondary products linked to\n",
    "secondary_dict= {\n",
    "    \"Calabazas\": [1,2],\n",
    "    \"Hinojo\": [0,2],\n",
    "    \"Sesamo\": [3,1],\n",
    "    \"Girasol\": [2,4],\n",
    "    \"Amapola\": [3,2]\n",
    "}\n",
    "\n",
    "# Matrix n_prod*n_prices collecting the possible prices for each product. Prices are in ascending order\n",
    "prices = [[6.5, 8, 9.5, 11],\n",
    "          [11., 12, 13, 16],\n",
    "          [20., 21, 22, 25],\n",
    "          [27., 29, 31, 37],\n",
    "          [40., 41, 44, 48]]\n",
    "# Production cost of the products\n",
    "cost = [4.75, 9.75, 13.5, 15.75, 15.75]\n",
    "\n",
    "#sarebbe interessante anche prendere da file il tutto così da cambiare tutto più facilmente\n",
    "#calcolo i margini dai cost mi sembra più sensato e anche più veloce se dobbiamo cambiare continuamente\n",
    "# Computation of margins linked to each product for a particular choice of price\n",
    "cost2 = np.tile(np.array([cost]).transpose(), (1, 4))\n",
    "margins = np.array(prices)-cost2\n",
    "# Creation of the 5 objects of Product class\n",
    "for i in range (5):\n",
    "    products.append(Product(prices[i], i, nameofproduct[i],margins[i]))\n",
    "\n",
    "# Parameter for the computation of the click probability on the SECOND secondary product\n",
    "lambda_q = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 1: Young and Inexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_1 = {\n",
    "    \"mean\": [8.5, 13, 21.5, 28, 40],\n",
    "    \"std\": [1, 1.5, 2, 2.5, 3]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_1 = [[0, 0.6, 0, 0, 0],\n",
    "                   [0.4, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0.2, 0],\n",
    "                   [0, 0, 0.3, 0, 0.1],\n",
    "                   [0, 0, 0.2, 0.4, 0]]\n",
    "prob_lambda_1 = lambda_correct(np.matrix(probabilities_1), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_1 = [15, 15, 10, 5, 5]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_1 = [2, 1, .5, .2, .1]\n",
    "\n",
    "user1 = UserCat(alphas_1, res_price_params_1, poisson_lambda_1, prob_lambda_1, 'Young and Not Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 2: Old and Inexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_2 = {\n",
    "    \"mean\": [9, 14, 25, 32, 44],\n",
    "    \"std\": [3, 1.5, 2.5, 3.5, 4]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_2 = [[0, 0.4, 0, 0, 0],\n",
    "                 [0.3, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.4, 0],\n",
    "                 [0, 0, 0.4, 0, 0.2],\n",
    "                 [0, 0, 0.4, 0.2, 0]]\n",
    "prob_lambda_2 = lambda_correct(np.matrix(probabilities_2), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_2 = [7, 12, 12, 12, 7]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_2 = [0.6, 1.1, 2, 0.9, 0.5]\n",
    "\n",
    "user2 = UserCat(alphas_2, res_price_params_2, poisson_lambda_2, prob_lambda_2, 'Old and Not Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 3: Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_3 = {\n",
    "    \"mean\": [7.5, 12, 23, 37, 49],\n",
    "    \"std\": [1.5, 1.5, 2, 4, 3.5]\n",
    "}\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_3 = [[0, 0.3, 0, 0, 0],\n",
    "                 [0.3, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.5, 0],\n",
    "                 [0, 0, 0.2, 0, 0.6],\n",
    "                 [0, 0, 0.3, 0.5, 0]]\n",
    "prob_lambda_3 = lambda_correct(np.matrix(probabilities_3), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_3 = [5, 5, 10, 15, 15]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_3 = [0.1, 0.2, 0.5, 1.5, 1.2]\n",
    "\n",
    "user3 = UserCat(alphas_3, res_price_params_3, poisson_lambda_3, prob_lambda_3, 'Expert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Category 0: Aggregated demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the distribution describing the reservation price\n",
    "res_price_params_0 = {\n",
    "    \"mean\": [9, 13, 22, 35, 42],\n",
    "    \"std\": [3, 1.5, 2, 2.5, 2.5]\n",
    "}\n",
    "\n",
    "# Matrix collecting the graph_weights describing mechanism of click on secondary products\n",
    "probabilities_0 = [[0, 0.5, 0, 0, 0],\n",
    "                 [0.4, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0.4, 0],\n",
    "                 [0, 0, 0.5, 0, 0.4],\n",
    "                 [0, 0, 0.2, 0.4, 0]]\n",
    "prob_lambda_0 = lambda_correct(np.matrix(probabilities_0), secondary_dict, lambda_q)\n",
    "# Parameter of the Dirichlet for the alphas ratio sampling\n",
    "alphas_0 = [10, 10, 10, 10, 10]\n",
    "# Parameter of the Poisson distribution determining the number of product bought\n",
    "# ! we considered a trasleted Poisson in 1, to avoid the case of 0 items bought, so\n",
    "#   the mean is poisson_lambda+1\n",
    "poisson_lambda_0 = [1.5, 1, .5, .4, .2]\n",
    "\n",
    "user0 = UserCat(alphas_0, res_price_params_0, poisson_lambda_0, prob_lambda_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMON\n",
    "# probability distribution of the features\n",
    "# the following list has to be interpreted in the following way:\n",
    "# values_i is the parameter of the bernoulli for feature i; in our case feature0 is 1(0) for Expert(Not Expert)\n",
    "# while feature1 is 1(0) for Old(Young)\n",
    "feature_prob = [0.3, 0.4]\n",
    "# CASE WITH 3 USERS :\n",
    "# list of users \n",
    "users3 = [user1, user2, user3]\n",
    "feature_matrix3 = np.array([[0, 1], [2, 2]]) # values represent the label of the User Category\n",
    "env3 = Environment(users3, products, secondary_dict, feature_matrix3, feature_prob)\n",
    "\n",
    "# CASE WITH AGGREGATED USER :\n",
    "users0 = [user0]\n",
    "feature_matrix0 = np.array([[0, 0], [0, 0]])\n",
    "env = Environment(users0, products, secondary_dict, feature_matrix0, feature_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upper Bound for Cumulative Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_ts = TS_regret(env, 365, 1e-3)\n",
    "ub_ucb = ucb_regret(env, 365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Optimal Reward e Optimal Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Aggregated User ----------------------------\n",
      "Aggregated Optimal Reward : 19.470 - Optimal Combination : [1, 1, 0, 2, 0]\n",
      "-------------------------- 3 classes Users ----------------------------\n",
      "Aggregated Optimal Reward : 25.122 VS Disaggregated Optimal Reward : 26.899\n",
      "Aggregated Optimal Price combination : [0, 1, 0, 1, 0]\n",
      "Optimal price combinations with users divided by category:\n",
      "Young and Not Expert - [1, 1, 0, 0, 0] - 10.046397\n",
      "Old and Not Expert - [0, 1, 2, 0, 0] - 26.956458\n",
      "Expert - [0, 0, 0, 2, 2] - 50.439718\n"
     ]
    }
   ],
   "source": [
    "aggr_opt_reward, aggr_opt_comb = env.optimal_reward()\n",
    "print('-------------------------- Aggregated User ----------------------------')\n",
    "print( 'Aggregated Optimal Reward : %.3f - Optimal Combination : %s' %(aggr_opt_reward, aggr_opt_comb))\n",
    "print('-------------------------- 3 classes Users ----------------------------')\n",
    "opt_rewards_array, opt_comb_list = env3.optimal_reward(Disaggregated=True)\n",
    "aggr_opt_reward, aggr_opt_comb = env3.optimal_reward()\n",
    "dis_opt_reward = np.sum(np.array(env3.user_cat_prob)*opt_rewards_array)\n",
    "print('Aggregated Optimal Reward : %.3f VS Disaggregated Optimal Reward : %.3f' %(aggr_opt_reward, dis_opt_reward))\n",
    "print('Aggregated Optimal Price combination : %s' %aggr_opt_comb)\n",
    "print('Optimal price combinations with users divided by category:')\n",
    "for i, user in enumerate(env3.users):\n",
    "    print('%s - %s - %f' %(user.category, str(opt_comb_list[i]), opt_rewards_array[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 : Greedy Algorithm Functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Reward and Optimal Combination found by Greedy optimizer:\n",
      "{'expected_reward': 19.470002473907506, 'combination': [1, 1, 0, 2, 0]}\n",
      "Thoeretica Optimal Reward and Optimal Combination:\n",
      "{expected_reward : 19.470002, combination : [1, 1, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "greedy_optimizer = Greedy_optimizer(env)\n",
    "print('Optimal Reward and Optimal Combination found by Greedy optimizer:')\n",
    "print(greedy_optimizer.run())\n",
    "print('Thoeretica Optimal Reward and Optimal Combination:')\n",
    "print('{expected_reward : %lf, combination : %s'%env.optimal_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values found by the Greedy Optimizer\n",
      "Optimal Expected Rewards : [10.046397, 26.956458, 50.439718]\n",
      "Optimale Combinations: [[1, 1, 0, 0, 0], [0, 1, 2, 0, 0], [0, 0, 0, 2, 2]]\n",
      "Total Expected Reward : 26.899210\n",
      "\n",
      "Theoretical Optimal Values\n",
      "Optimal Expected Rewards : [10.046397, 26.956458, 50.439718]\n",
      "Optimale Combinations: [[1, 1, 0, 0, 0], [0, 1, 2, 0, 0], [0, 0, 0, 2, 2]]\n",
      "Total Expected Reward : 26.899210\n"
     ]
    }
   ],
   "source": [
    "greedy_3 = Greedy_optimizer(env3)\n",
    "greedy_3.run()\n",
    "\n",
    "rew0, comb0 = greedy_3.run(group_list=[[0,0]]).values()\n",
    "rew1, comb1 = greedy_3.run(group_list=[[0,1]]).values()\n",
    "rew2, comb2 = greedy_3.run(group_list=[[1,0], [1,1]]).values()\n",
    "print('Optimal Values found by the Greedy Optimizer')\n",
    "print('Optimal Expected Rewards : [%f, %f, %f]\\nOptimale Combinations: [%s, %s, %s]' %(rew0,rew1,rew2,comb0,comb1,comb2))\n",
    "print('Total Expected Reward : %f' %(np.sum(np.array((rew0,rew1,rew2))*env3.user_cat_prob)))\n",
    "print('\\nTheoretical Optimal Values')\n",
    "rewards, combinations = env3.optimal_reward(Disaggregated=True)\n",
    "print('Optimal Expected Rewards : [%f, %f, %f]' %(rewards[0], rewards[1], rewards[2]))\n",
    "print('Optimale Combinations: [%s, %s, %s]' %(combinations[0], combinations[1], combinations[2] ))\n",
    "print('Total Expected Reward : %f' %(np.sum(rewards*env3.user_cat_prob)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 : Uncertain Convertion Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "learner_TS3 = Step3_TS(env, initial_beta_CR, learning_rate = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS3.reward_history = []\n",
    "learner_TS3.price_comb_history = []\n",
    "learner_TS3.cr_matrix_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS3.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS3 = learner_TS3.opt_reward\n",
    "collected_rewards_TS3 = learner_TS3.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step3_TS', 'rb') as f: \n",
    "    learner_TS3 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS3 = learner_TS3.opt_reward\n",
    "collected_rewards_TS3 = learner_TS3.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS3 - collected_rewards_TS3, axis=0)), 'r') \n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS3 - collected_rewards_TS3, axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_TS3 = np.cumsum(opt_reward_TS3 - collected_rewards_TS3, axis = 1)\n",
    "mean_cum_R_TS3 = np.mean(cum_R_TS3, axis = 0)\n",
    "std_dev_TS3 = np.std(cum_R_TS3, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS3)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS3-std_dev_TS3, mean_cum_R_TS3+std_dev_TS3, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS3, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS3, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS3 = mean_cum_R_TS3/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS3[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CR_TS3 = np.mean(learner_TS3.cr_matrix_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %str(mean_CR_TS3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb3 = step3_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb3.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb3\", 'wb') as f1:\n",
    "    pickle.dump(ucb3, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb3\", 'rb') as f1:\n",
    "    ucb3 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_ucb1_collected_rewards = ucb3.collected_rewards\n",
    "step3_ucb1_R = ucb3.regret\n",
    "# plot of the result\n",
    "mean_step3_ucb1_R = np.mean(step3_ucb1_R, axis=0)\n",
    "std_dev_step3_ucb1 = np.std(step3_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step3_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step3_ucb1_R-std_dev_step3_ucb1, mean_step3_ucb1_R+std_dev_step3_ucb1, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step3_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb3 = mean_step3_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb3[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb3.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb3.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb3.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 : Uncertain conversion rates, alpha ratio and number of products sold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "initial_beta_alpha = np.ones((2,5))\n",
    "initial_n_prod_data = np.ones((2,5))\n",
    "learner_TS4 = Step4_TS(env3, initial_beta_CR, initial_beta_alpha, initial_n_prod_data, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS4.reward_history = []\n",
    "learner_TS4.price_comb_history = []\n",
    "learner_TS4.cr_matrix_list = []\n",
    "learner_TS4.alpha_ratios_list = []\n",
    "learner_TS4.n_prod_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS4.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS4 = learner_TS4.opt_reward\n",
    "collected_rewards_TS4 = learner_TS4.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step4_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS4, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step4_TS', 'rb') as f: \n",
    "    learner_TS4 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS4 = learner_TS4.opt_reward\n",
    "collected_rewards_TS4 = learner_TS4.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS4 - collected_rewards_TS4, axis=0)), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS4 - collected_rewards_TS4, axis=0), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_TS4 = np.cumsum(opt_reward_TS4 - collected_rewards_TS4, axis = 1)\n",
    "mean_cum_R_TS4 = np.mean(cum_R_TS4, axis = 0)\n",
    "std_dev_TS4 = np.std(cum_R_TS4, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS4)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS4-std_dev_TS4, mean_cum_R_TS4+std_dev_TS4, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS4, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS4, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS4 = mean_cum_R_TS4/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS4[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_alpha_TS4 = np.mean(np.array(learner_TS4.alpha_ratios_list), axis = 0)\n",
    "mean_n_prod_TS4 = np.mean(np.array(learner_TS4.n_prod_list), axis = 0)\n",
    "mean_CR_TS4 = np.mean(learner_TS4.cr_matrix_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %mean_CR_TS4)\n",
    "print('\\nAlpha Ratios : %s' % mean_alpha_TS4)\n",
    "print('\\nMean Number of product sold : %s' %mean_n_prod_TS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))\n",
    "print('\\nAlpha Ratios : %s' %env.alpha_ratios[0] )\n",
    "print('\\nMean Number of product sold : %s' %(env.users[0].poisson_lambda+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb4 = step4_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb4.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb4\", 'wb') as f1:\n",
    "    pickle.dump(ucb4, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb4\", 'rb') as f1:\n",
    "    ucb4 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4_ucb1_collected_rewards = ucb4.collected_rewards\n",
    "step4_ucb1_R = ucb4.regret\n",
    "# plot of the result\n",
    "mean_step4_ucb1_R = np.mean(step4_ucb1_R, axis=0)\n",
    "std_dev_step4_ucb1 = np.std(step4_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step4_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step4_ucb1_R-std_dev_step4_ucb1, mean_step4_ucb1_R+std_dev_step4_ucb1, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step4_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb4 = mean_step4_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb4[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb4.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb4.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb4.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5 : Uncertain conversion rates and graph weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a_cr = np.ones((5,4))\n",
    "b_cr = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a_cr, b_cr])\n",
    "a_gw = np.ones((5,2))\n",
    "b_gw = np.ones((5,2))\n",
    "initial_beta_gw = np.array([a_gw, b_gw])\n",
    "learner_TS5 = Step5_TS(env, initial_beta_CR, initial_beta_gw, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS5.reward_history = []\n",
    "learner_TS5.price_comb_history = []\n",
    "learner_TS5.cr_matrix_list = []\n",
    "learner_TS5.graph_weights_list = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS5.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS5 = learner_TS5.opt_reward\n",
    "collected_rewards_TS5 = learner_TS5.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step5_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS5, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step5_TS', 'rb') as f: \n",
    "    learner_TS5 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS5 = learner_TS5.opt_reward\n",
    "collected_rewards_TS5 = learner_TS5.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS5 - collected_rewards_TS5, axis=0)), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS5 - collected_rewards_TS5, axis=0), 'r')  \n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_TS5 = np.cumsum(opt_reward_TS5 - collected_rewards_TS5, axis = 1)\n",
    "mean_cum_R_TS5 = np.mean(cum_R_TS5, axis = 0)\n",
    "std_dev_TS5 = np.std(cum_R_TS5, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS5)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS5-std_dev_TS5, mean_cum_R_TS5+std_dev_TS5, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS5, color = 'green')\n",
    "plt.plot(np.mean(collected_rewards_TS5, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_TS5 = mean_cum_R_TS5/ub_ts\n",
    "print('Last iteration Ratio is : %f' %ratio_list_TS5[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Estimation of Uncertain Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_CR_TS5 = np.mean(learner_TS5.cr_matrix_list, axis = 0)\n",
    "mean_GW_TS5 = np.mean(learner_TS5.graph_weights_list, axis = 0)\n",
    "\n",
    "print('Conversion Rates:\\n%s' %mean_CR_TS5)\n",
    "print('\\nGraph Weights : %s' %mean_GW_TS5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('THEORETICAL VALUES:\\n\\nConversion Rates :\\n%s' %np.matrix(env.theoretical_values['conversion_rates'][0]))\n",
    "print('\\nGraph Weights :\\n%s' %env.users[0].probabilities )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_reward = env.optimal_reward()[0]\n",
    "ucb5 = step5_ucb1(len(prices), len(prices[0]), prices, env)\n",
    "for _ in range(n_runs):\n",
    "    ucb5.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb5\", 'wb') as f1:\n",
    "    pickle.dump(ucb5, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb5\", 'rb') as f1:\n",
    "    ucb5 = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step5_ucb1_collected_rewards = ucb5.collected_rewards\n",
    "step5_ucb1_R = ucb5.regret\n",
    "# plot of the result\n",
    "mean_step5_ucb1_R = np.mean(step5_ucb1_R, axis=0)\n",
    "std_dev_step5_ucb1 = np.std(step5_ucb1_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_step5_ucb1_R)\n",
    "plt.fill_between(range(n_days), mean_step5_ucb1_R-std_dev_step5_ucb1, mean_step5_ucb1_R+std_dev_step5_ucb1, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.axhline(opt_reward, color = 'green')\n",
    "plt.plot(np.mean(step5_ucb1_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio with respect to theoretical upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_list_ucb5 = mean_step5_ucb1_R/ub_ucb\n",
    "print('Last iteration Ratio is : %f' %ratio_list_ucb5[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last n pulled arms\n",
    "print(\"Last n pulled arms:\")\n",
    "np.array(ucb5.pulled[-10:-1], dtype=np.int32)[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ucb1 most pulled arms\n",
    "combinations_data = [[] for i in range(1024)]\n",
    "for i1 in range(4):\n",
    "    for i2 in range(4):\n",
    "        for i3 in range(4):\n",
    "            for i4 in range(4):\n",
    "                for i5 in range(4):\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append([i1, i2, i3, i4, i5])\n",
    "                    c = np.array(np.array(ucb5.pulled, dtype=np.int32)[:, 0].tolist()) == [i1, i2, i3, i4, i5]\n",
    "                    c = np.prod(c, axis=1)\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(np.count_nonzero(c))\n",
    "                    combinations_data[i1*(4**4) + i2*(4**3) + i3*(4**2) + i4*(4**1) + i5*(4**0)].append(env.expected_reward([i1, i2, i3, i4, i5]))\n",
    "                    x = combinations_data\n",
    "result = []\n",
    "for i in range(20):\n",
    "    result.append(x[np.argmax(np.array(x)[:, 1])])\n",
    "    x = np.delete(x, np.argmax(np.array(x)[:, 1]), axis=0).tolist()\n",
    "print(\"Optimal arms combination:\")\n",
    "print(env.optimal_reward()[1], env.optimal_reward()[0])\n",
    "print(\"\\n\\nUcb1 most pulled arms:\")\n",
    "print(\"(arms combination), (n° pulls), (exp rew)\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb5.print_estimations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6 : Abrupt Changes in Demand Curve with Uncertain Conversion Rates and Graph Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Abrupt Changes Setting Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# ABRUPT CHANGE SETTING #\n",
    "#########################\n",
    "n_days = 365\n",
    "\n",
    "# We suppose to start in September, with relative high demand. Our hypothesis is that every season we have an\n",
    "# abrupt change in demand curve\n",
    "changes_dict ={ # BASE CASE: AUTUMN\n",
    "    90 : {\"mean\": [10.1, 13.8, 23.2, 36.4, 43.7], \"std\": [3, 1.5, 2, 2.5, 2.5]}, # WINTER\n",
    "    180 : {\"mean\": [9, 13, 22, 35, 42], \"std\": [3, 1.5, 2, 2.5, 2.5]}, # SPRING\n",
    "    270 : {\"mean\": [8.2, 11.7, 20.5, 31.6, 39.1], \"std\": [3, 1.5, 2, 2.5, 2.5]} # SUMMER\n",
    "}\n",
    "opt_reward = env.optimal_reward()[0]\n",
    "opt_reward_evolution = np.zeros(n_days)\n",
    "original_res_price_param = copy.deepcopy(env.users[0].res_price_params)\n",
    "for t in range(n_days):\n",
    "    if t in changes_dict.keys(): \n",
    "        env.abrupt_change_deterministic([changes_dict[t]])\n",
    "        opt_reward = env.optimal_reward()[0]\n",
    "    opt_reward_evolution[t] = opt_reward\n",
    "\n",
    "env.abrupt_change_deterministic([original_res_price_param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - SW UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 100\n",
    "daily_users = 100\n",
    "\n",
    "sw = int(3*np.sqrt(n_days))\n",
    "sw_ucb = step6_sw_ucb(len(prices), len(prices[0]), prices, env, changes_dict, sw)\n",
    "for _ in range(n_runs):\n",
    "    sw_ucb.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"sw_ucb\", 'wb') as f1:\n",
    "    pickle.dump(sw_ucb, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"sw_ucb\", 'rb') as f1:\n",
    "    sw_ucb = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_ucb_collected_rewards = sw_ucb.collected_rewards\n",
    "sw_ucb_R = sw_ucb.regret\n",
    "# plot of the result\n",
    "mean_sw_ucb_R = np.mean(sw_ucb_R, axis=0)\n",
    "std_sw_ucb = np.std(sw_ucb_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_sw_ucb_R)\n",
    "plt.fill_between(range(n_days), mean_sw_ucb_R-std_sw_ucb, mean_sw_ucb_R+std_sw_ucb, alpha=0.4)\n",
    "changes_time = list(changes_dict.keys())\n",
    "changes_n = len(changes_time)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_sw_ucb_R)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.plot(opt_reward_evolution, color = 'green')\n",
    "plt.plot(np.mean(sw_ucb_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Change Detection UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change detection hyper parameters\n",
    "m = 50\n",
    "eps = 0.02\n",
    "h = 0.3\n",
    "cd_ucb = Step6_CD(len(prices), len(prices[0]), prices, env, changes_dict, m, eps, h)\n",
    "# run setting (n_days fixed before)\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    cd_ucb.run(n_days, daily_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"cd_ucb\", 'wb') as f1:\n",
    "    pickle.dump(cd_ucb, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"cd_ucb\", 'rb') as f1:\n",
    "    cd_ucb = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_ucb_collected_rewards = cd_ucb.collected_rewards\n",
    "cd_ucb_R = cd_ucb.regret\n",
    "# plot of the result\n",
    "mean_cd_ucb_R = np.mean(cd_ucb_R, axis=0)\n",
    "std_cd_ucb = np.std(cd_ucb_R, axis=0)/np.sqrt(n_runs)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(mean_cd_ucb_R)\n",
    "plt.fill_between(range(n_days), mean_cd_ucb_R-std_cd_ucb, mean_cd_ucb_R+std_cd_ucb, alpha=0.4)\n",
    "changes_time = list(changes_dict.keys())\n",
    "changes_n = len(changes_time)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_cd_ucb_R)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between optimal and expected reward\n",
    "plt.figure(0)\n",
    "plt.title(\"Reward - Optimal vs Collected\")\n",
    "plt.plot(opt_reward_evolution, color = 'green')\n",
    "plt.plot(np.mean(cd_ucb_collected_rewards, axis=0))\n",
    "plt.legend([\"Optimal Reward\", \"Mean Collected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Sliding Window TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "# CONVERSION RATES INITIAL VALUES\n",
    "a_cr = np.ones((5,4))*20\n",
    "b_cr = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a_cr, b_cr])\n",
    "# GRAPH WEIGHTS INITIAL VALUES\n",
    "a_gw = np.ones((5,2))\n",
    "b_gw = np.ones((5,2))\n",
    "initial_beta_gw = np.array([a_gw, b_gw])\n",
    "\n",
    "# Learner initializer\n",
    "learner_TS6 = Step6_TS_sw(env, initial_beta_CR, initial_beta_gw, sw, changes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define run setting (n_days fixed for all step 6 methods)\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS6.reward_history = []\n",
    "learner_TS6.price_comb_history = []\n",
    "learner_TS6.cr_matrix_list = []\n",
    "learner_TS6.graph_weights_list = []\n",
    "learner_TS6.CR_data_history = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS6.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS6 = learner_TS6.opt_reward\n",
    "collected_rewards_TS6 = learner_TS6.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step6_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS6, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step6_TS', 'rb') as f: \n",
    "    learner_TS6 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "cum_regret = np.cumsum(np.mean(opt_reward_evolution - collected_rewards_TS6, axis=0))\n",
    "plt.plot(cum_regret, 'blue', label = 'TS')\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(cum_regret)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "std_regret = np.std(opt_reward_evolution - collected_rewards_TS6, axis=0)\n",
    "plt.plot(std_regret, 'blue', label = 'TS') \n",
    "plt.vlines(changes_time, [0]*changes_n, [max(std_regret)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_TS6 = np.cumsum(opt_reward_evolution - collected_rewards_TS6, axis = 1)\n",
    "mean_cum_R_TS6 = np.mean(cum_R_TS6, axis = 0)\n",
    "std_dev_TS6 = np.std(cum_R_TS6, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS6)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS6-std_dev_TS6, mean_cum_R_TS6+std_dev_TS6, alpha=0.4)\n",
    "plt.vlines(changes_time, [0]*changes_n, [max(mean_cum_R_TS6)]*changes_n, color = 'red', label= 'changes time')\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.plot(opt_reward_evolution, color = 'green', label = \"Optimal Reward\")\n",
    "plt.plot(np.mean(collected_rewards_TS6, axis=0), label = \"Mean Expected Reward\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7 : Context generation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial assumptions for beta parameters (uniform distr. on [0, 1])\n",
    "a = np.ones((5,4))\n",
    "b = np.ones((5,4))\n",
    "initial_beta_CR = np.array([a, b])\n",
    "initial_beta_alpha = np.ones((2,5))\n",
    "initial_n_prod_data = np.ones((2,5))\n",
    "cg_confidence = 0.05\n",
    "learner_TS7 = Step7_TS(env3, initial_beta_CR, initial_beta_alpha, initial_n_prod_data, cg_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameter for the algorithm execution\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "# delete possible old informations form past runs \n",
    "learner_TS7.reward_history = []\n",
    "learner_TS7.context_history = []\n",
    "\n",
    "# execute the algorithm n_runs times\n",
    "for i in range(n_runs) :\n",
    "    learner_TS7.run(n_days, daily_users)\n",
    "\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS7 = learner_TS7.opt_reward\n",
    "collected_rewards_TS7 = learner_TS7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salvo la history su file in modo che siamo sicuri ti riuscire a recuperarla anche in un secondo momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step7_TS', 'wb') as f: \n",
    "    pickle.dump(learner_TS7, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per recuperare, invece, i risultati ottenuti in un secondo momento :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('step7_TS', 'rb') as f: \n",
    "    learner_TS7 = pickle.load(f)\n",
    "# collect all informations for the plot\n",
    "opt_reward_TS7 = learner_TS7.opt_reward\n",
    "collected_rewards_TS7 = learner_TS7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_TS7 - collected_rewards_TS7, axis=0)), 'r')\n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_TS7 - collected_rewards_TS7, axis=0), 'r') \n",
    "plt.legend([\"TS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_TS7 = np.cumsum(opt_reward_TS7 - collected_rewards_TS7, axis = 1)\n",
    "mean_cum_R_TS7 = np.mean(cum_R_TS7, axis = 0)\n",
    "std_dev_TS7 = np.std(cum_R_TS7, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_TS7)\n",
    "plt.fill_between(range(n_days), mean_cum_R_TS7-std_dev_TS7, mean_cum_R_TS7+std_dev_TS7, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_TS7, color = 'green')\n",
    "plt.axhline(aggr_opt_reward, color = 'red')\n",
    "plt.plot(np.mean(collected_rewards_TS7, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\",\"Aggregated Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the run parameters\n",
    "n_runs = 100\n",
    "daily_users = 100\n",
    "n_days = 365\n",
    "\n",
    "confidence = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb7 = step7_ucb1(len(prices), len(prices[0]), prices, env3, confidence)\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    ucb7.run(n_days, daily_users)\n",
    "\n",
    "opt_reward_ucb7 = ucb7.opt_reward\n",
    "collected_rewards_ucb7 = ucb7.reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result\n",
    "with open(\"ucb7\", 'wb') as f1:\n",
    "    pickle.dump(ucb7, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the result\n",
    "with open(\"ucb7\", 'rb') as f1:\n",
    "    ucb7 = pickle.load(f1)\n",
    "# collect all informations for the plot\n",
    "opt_reward_ucb7 = ucb7.opt_reward\n",
    "collected_rewards_ucb7 = ucb7.reward_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cumulative Regret Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.title(\"Cumulative Regret\")\n",
    "plt.plot(np.cumsum(np.mean(opt_reward_ucb7 - collected_rewards_ucb7, axis=0)), 'r')  #'r' stay for red, the color for the TS algorithm\n",
    "plt.legend([\"UCB1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Standard Deviation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.title(\"Regret's Standard Deviation\")\n",
    "plt.plot(np.std(opt_reward_ucb7 - collected_rewards_ucb7, axis=0), 'r')\n",
    "plt.legend([\"UCB1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_R_ucb7 = np.cumsum(opt_reward_ucb7 - collected_rewards_ucb7, axis = 1)\n",
    "mean_cum_R_ucb7 = np.mean(cum_R_ucb7, axis = 0)\n",
    "std_dev_ucb7 = np.std(cum_R_ucb7, axis=0)/np.sqrt(n_runs)\n",
    "plt.plot(mean_cum_R_ucb7)\n",
    "plt.fill_between(range(n_days), mean_cum_R_ucb7-std_dev_ucb7, mean_cum_R_ucb7+std_dev_ucb7, alpha=0.4)\n",
    "plt.title(\"Cumulative Regret and its Std. Deviation\")\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Cum_Regret(t)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison between Optimal and Expected Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "plt.title(\"Optimal VS Expected Reward\")\n",
    "plt.axhline(opt_reward_ucb7, color = 'green')\n",
    "plt.axhline(aggr_opt_reward, color = 'red')\n",
    "plt.plot(np.mean(collected_rewards_ucb7, axis=0))\n",
    "plt.xlabel(\"t (days)\")\n",
    "plt.ylabel(\"Expected Reward (t)\")\n",
    "plt.legend([\"Optimal Reward\",\"Aggregated Optimal Reward\", \"Mean Expected Reward\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
